{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a21edb-c87a-4a91-8a4e-46d654563fa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Cuda and installing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7382586-6157-41ba-988c-d25d0ba9984b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is correctly set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93f04c-3128-40d1-80cd-19d569d1a50d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing packages and downloading/cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909d8292-3b57-4725-80ec-85efd7f674ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import open_clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import  unicom\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "import faiss\n",
    "import gc\n",
    "from image_utils import worker_function, get_image_embeddings\n",
    "#from embedding_utils import convert_to_tensors, mean_pool_embeddings, normalize_embeddings, worker_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c35a89-b8ab-415d-a8c1-3cdb926b825a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c9f02-ffd9-4fce-a40d-5d34dfe39c59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Download the metadata CSV if it doesn't already exist\n",
    "def download_metadata_csv(metadata_url, metadata_file_path):\n",
    "    if not os.path.exists(metadata_file_path):\n",
    "        response = requests.get(metadata_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(metadata_file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Metadata CSV file saved as {metadata_file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download the metadata file. Status code: {response.status_code}\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Metadata file already exists at {metadata_file_path}\")\n",
    "\n",
    "# Step 2: Convert the metadata CSV file to DataFrame\n",
    "def convert_metadata_to_dataframe(metadata_file_path):\n",
    "    df = pd.read_csv(metadata_file_path)\n",
    "    df.rename(columns={\"fileNameAsDelivered\": \"image_filename\", \"scientificName\": \"scientificName\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Helper function to download a single image\n",
    "def download_single_image(row, chunk_base_url, chunk_count, output_dir):\n",
    "    image_filename = row['image_filename']\n",
    "    image_downloaded = False\n",
    "    for chunk_index in range(chunk_count):\n",
    "        image_url = f\"{chunk_base_url}chunk_{chunk_index}/{image_filename}\"\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            image_path = os.path.join(output_dir, image_filename)\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded and saved: {image_filename} from chunk_{chunk_index}\")\n",
    "            image_downloaded = True\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Image not found in chunk_{chunk_index}: {image_filename}\")\n",
    "    if not image_downloaded:\n",
    "        print(f\"Failed to download {image_filename} from all chunks.\")\n",
    "\n",
    "# Step 3: Download images from the specified chunks using multithreading\n",
    "def download_images(df, chunk_base_url, chunk_count, output_dir, max_threads=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        # Submit download tasks for each row\n",
    "        futures = [executor.submit(download_single_image, row, chunk_base_url, chunk_count, output_dir) for _, row in df.iterrows()]\n",
    "        \n",
    "        # Wait for all the futures to complete\n",
    "        for future in futures:\n",
    "            future.result()  # This will raise an exception if the thread encountered any issues\n",
    "\n",
    "# Step 4: Filter existing images and remove missing records\n",
    "def filter_existing_images_with_scientific_name(df, output_dir):\n",
    "    df['file_exists'] = df['image_filename'].apply(lambda x: os.path.exists(os.path.join(output_dir, x)))\n",
    "    df_cleaned = df[df['file_exists']].copy()\n",
    "    df_cleaned.drop(columns=['file_exists'], inplace=True)\n",
    "    return df_cleaned\n",
    "\n",
    "# Step 5: Save the DataFrame to CSV\n",
    "def save_dataframe_to_csv(df, csv_file_path=\"cleaned_images_with_scientific_names.csv\"):\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"DataFrame saved to {csv_file_path}\")\n",
    "\n",
    "# Main execution\n",
    "metadata_url = \"https://huggingface.co/datasets/sammarfy/VLM4Bio/resolve/main/datasets/Bird/metadata/metadata_10k.csv\"\n",
    "metadata_file_path = \"metadata_10k.csv\"\n",
    "output_dir = \"downloaded_images\"\n",
    "chunk_base_url = \"https://huggingface.co/datasets/sammarfy/VLM4Bio/resolve/main/datasets/Bird/\"\n",
    "chunk_count = 5  # We have chunk_0 to chunk_4\n",
    "\n",
    "# Execute steps\n",
    "download_metadata_csv(metadata_url, metadata_file_path)\n",
    "df = convert_metadata_to_dataframe(metadata_file_path)\n",
    "download_images(df, chunk_base_url, chunk_count, output_dir, max_threads=10)  # Set max_threads to the desired number\n",
    "df_cleaned = filter_existing_images_with_scientific_name(df, output_dir)\n",
    "save_dataframe_to_csv(df_cleaned, \"cleaned_images_with_scientific_names.csv\")\n",
    "\n",
    "# Get unique labels from df[\"scientific_name\"]\n",
    "labels = df_cleaned[\"scientificName\"].unique().tolist()\n",
    "\n",
    "\n",
    "#combine chunk 0-4\n",
    "# multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e5f93-44b0-47d9-bf1f-fd3662dbf750",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfa79a-8dd0-473d-8251-1f6b27edf103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"cleaned_images_with_scientific_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056bd7a-038a-4bd8-8e5f-51f59425e36c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract Genus and then Split data on genus column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1c17a-3902-41a5-8f00-40dce8ff513d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract the genus from the scientific name (first part before space)\n",
    "def extract_genus(scientific_name):\n",
    "    return scientific_name.split()[0]\n",
    "\n",
    "def split_create_genus():\n",
    "    # Apply genus extraction to df_cleaned\n",
    "    global train_df, test_df\n",
    "    df_cleaned['genus'] = df_cleaned['scientificName'].apply(extract_genus)\n",
    "\n",
    "    # Count the number of samples per genus\n",
    "    genus_counts = df_cleaned['genus'].value_counts()\n",
    "\n",
    "    # Splitting the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n",
    "\n",
    "    # Checking the resulting shapes of the train and test sets\n",
    "    print(f\"Training set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    \n",
    "split_create_genus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe7639-ce52-4261-80bf-d7a667c744d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2526c38-b3fb-492c-aecb-748fd421989f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zero Shot test, Split data into testing (70:30), Average Pooling (Mean of Embeddings) for Florence Only, Getting highest Score, Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8f80c-7739-4058-9f63-f0e40a652d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be0385d-0930-496d-8f10-04b600010e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Florence-2 Large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca48ff48-b80b-4d6c-9d8d-abbe1265f3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Set up device and torch dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load Florence-2 Large model and processor\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "model = model.eval()\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df07584b-3de3-4ec1-bbb8-9150f5e76310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for handling image loading\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_filenames, image_folder):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.image_folder = image_folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_filename)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return image, image_filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing image {image_filename}: {e}\")\n",
    "            return None, image_filename\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    images, filenames = zip(*batch)\n",
    "    # Filter out None images from the batch\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_filenames = [fname for img, fname in zip(images, filenames) if img is not None]\n",
    "\n",
    "    if len(valid_images) > 0:\n",
    "        # Pass the raw images directly to the processor here (on CPU)\n",
    "        inputs = processor(images=valid_images, return_tensors=\"pt\")\n",
    "        # Ensure the input tensor is in float16 if the model expects it\n",
    "        inputs = {k: v.to(dtype=torch.float16) if model.dtype == torch.float16 else v for k, v in inputs.items()}\n",
    "        return inputs, valid_filenames\n",
    "    else:\n",
    "        return None, valid_filenames\n",
    "\n",
    "# Function to generate image embeddings with tqdm progress bar and dtype correction\n",
    "def image_embedding(df, batch_size=16, num_workers=3):  # You can use 3 workers now\n",
    "    image_filenames = df['image_filename'].tolist()\n",
    "    image_folder = \"downloaded_images\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = ImageDataset(image_filenames, image_folder)\n",
    "    \n",
    "    # Create DataLoader with the custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    results = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=len(dataloader), desc=\"Processing Batches\", unit=\"batch\") as pbar:\n",
    "        # Process using DataLoader\n",
    "        for inputs, batch_filenames in dataloader:\n",
    "            if inputs is not None:\n",
    "                with torch.inference_mode():\n",
    "                    # Move the inputs to the GPU and ensure they match the model's precision\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    # Ensure the embeddings computation uses consistent dtype\n",
    "                    embeddings = model._encode_image(inputs[\"pixel_values\"]).cpu().numpy()\n",
    "                    results.append(embeddings)\n",
    "                    filenames.extend(batch_filenames)\n",
    "            pbar.update(1)  # Update progress bar after processing each batch\n",
    "                \n",
    "    # Flatten the list of results and store in df \n",
    "    if results:\n",
    "        embeddings = np.vstack(results)\n",
    "        df['image_embeddings'] = list(embeddings)\n",
    "    return df\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef5d642-3646-4430-a673-c9a3251beee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/694 [00:00<?, ?batch/s]You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n",
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 694/694 [03:49<00:00,  3.03batch/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure this block is only for execution, after moving class/function definitions out\n",
    "if __name__ == \"__main__\":\n",
    "    df = df_cleaned  # Replace with your actual dataframe\n",
    "    df = image_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cef8c40-18b0-4886-8ee6-a98c3ef33cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d31db0-f34c-49f6-bb38-58a54ebe77f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1bbe9b-f1fa-47c0-ad4d-b0d9f242190c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a2973-9cfd-4575-9801-ca6590a14518",
   "metadata": {},
   "source": [
    "# Creating embeddings, normalizing L2, Builiding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51608b54-41d2-43b9-92df-c5f9e84c83ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned.to_pickle(\"df_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bddf70a-d916-443e-ac2e-18d323d4c780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_pickle(\"df_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68174a4f-5850-4db1-a3e2-3adaa362203c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspiration frread_pickle code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab71ef-e9b8-4d44-8f72-24035b058fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "train_embeddings_tensor = torch.tensor(np.stack(train_df['image_embeddings'].values), device='cuda')\n",
    "test_embeddings_tensor = torch.tensor(np.stack(test_df['image_embeddings'].values), device='cuda')\n",
    "\n",
    "# Mean pool \n",
    "train_embeddings_mean = train_embeddings_tensor.mean(dim=1)  \n",
    "test_embeddings_mean = test_embeddings_tensor.mean(dim=1)    \n",
    "\n",
    "# Normalize \n",
    "train_embeddings_normalized = train_embeddings_mean / train_embeddings_mean.norm(dim=1, keepdim=True)\n",
    "test_embeddings_normalized = test_embeddings_mean / test_embeddings_mean.norm(dim=1, keepdim=True)\n",
    "\n",
    "def find_best_match(test_embedding, train_embeddings_normalized, train_labels):\n",
    "    cosine_similarities = torch.matmul(test_embedding, train_embeddings_normalized.T)  # Shape [1, N]\n",
    "\n",
    "    max_similarity_index = torch.argmax(cosine_similarities).item()\n",
    "\n",
    "    return train_labels[max_similarity_index], cosine_similarities.max().item()\n",
    "\n",
    "def apply_best_match(test_embeddings_normalized, train_embeddings_normalized, train_df, label_column):\n",
    "    train_labels = train_df[label_column].tolist()\n",
    "    results = []\n",
    "\n",
    "    for test_embedding in tqdm(test_embeddings_normalized, desc=\"Processing Test Embeddings\"):\n",
    "        test_embedding = test_embedding.unsqueeze(0)\n",
    "        result = find_best_match(test_embedding, train_embeddings_normalized, train_labels)\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['highest_individual_name', 'highest_individual_score'])\n",
    "\n",
    "    results_df['true_label'] = test_df[label_column].tolist()\n",
    "\n",
    "    accuracy_individual = np.mean(results_df['highest_individual_name'] == results_df['true_label'])\n",
    "\n",
    "    print(f\"Accuracy based on highest individual cosine similarity: {accuracy_individual * 100:.2f}%\")\n",
    "\n",
    "    return results_df, accuracy_individual\n",
    "\n",
    "label_column = 'genus'  \n",
    "results_df, accuracy_individual = apply_best_match(\n",
    "    test_embeddings_normalized, train_embeddings_normalized, train_df, label_column\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d98f2-b60e-470e-b6a6-aa47ec785002",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Minor Changes to the code for better float16 conversion, avoiding division with 0 , cuda flushing, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57fbcb9c-ac14-40d4-8292-533ddca30f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Converting train and test embeddings to torch tensors...\n",
      "Step 2: Mean pooling the embeddings...\n",
      "Step 3: Normalizing the embeddings...\n",
      "Starting batch processing for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Embeddings in Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch from index 0 to 512...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Embeddings in Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing CUDA memory...\n",
      "Processing batch from index 512 to 1024...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 1024 to 1536...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 1536 to 2048...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 2048 to 2560...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 2560 to 3072...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 3072 to 3584...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 3584 to 3661...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Creating results DataFrame...\n",
      "Calculating accuracy...\n",
      "Accuracy based on highest individual cosine similarity: 64.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Step 1: Converting train and test embeddings to torch tensors...\")\n",
    "train_embeddings_tensor = torch.tensor(\n",
    "    np.stack(train_df['image_embeddings'].values), device='cuda', dtype=torch.float16\n",
    ")\n",
    "test_embeddings_tensor = torch.tensor(\n",
    "    np.stack(test_df['image_embeddings'].values), device='cuda', dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Step 2: Mean pooling the embeddings...\")\n",
    "train_embeddings_mean = train_embeddings_tensor.mean(dim=1) \n",
    "test_embeddings_mean = test_embeddings_tensor.mean(dim=1)    \n",
    "\n",
    "print(\"Step 3: Normalizing the embeddings...\")\n",
    "epsilon = 1e-8\n",
    "train_embeddings_normalized = train_embeddings_mean / (train_embeddings_mean.norm(dim=1, keepdim=True) + epsilon)\n",
    "test_embeddings_normalized = test_embeddings_mean / (test_embeddings_mean.norm(dim=1, keepdim=True) + epsilon)\n",
    "\n",
    "def apply_best_match_with_batches(test_embeddings_normalized, train_embeddings_normalized, train_df, label_column, batch_size=512):\n",
    "    train_labels = train_df[label_column].tolist()\n",
    "    num_test_samples = test_embeddings_normalized.size(0)\n",
    "    results = []\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_test_samples, batch_size), desc=\"Processing Test Embeddings in Batches\"):\n",
    "        print(f\"Processing batch from index {start_idx} to {min(start_idx + batch_size, num_test_samples)}...\")\n",
    "        end_idx = min(start_idx + batch_size, num_test_samples)\n",
    "        batch_embeddings = test_embeddings_normalized[start_idx:end_idx]\n",
    "\n",
    "        print(\"Calculating cosine similarities...\")\n",
    "        cosine_similarities = torch.matmul(batch_embeddings, train_embeddings_normalized.T)\n",
    "\n",
    "        print(\"Finding highest similarity indices...\")\n",
    "        max_similarity_indices = torch.argmax(cosine_similarities, dim=1)\n",
    "        max_similarity_values = torch.max(cosine_similarities, dim=1).values\n",
    "\n",
    "        print(\"Appending results...\")\n",
    "        for idx, sim_value in zip(max_similarity_indices, max_similarity_values):\n",
    "            results.append((train_labels[idx.item()], sim_value.item()))\n",
    "\n",
    "        # Manually clearing CUDA memory cache only\n",
    "        print(\"Clearing CUDA memory...\")\n",
    "        del cosine_similarities\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Creating results DataFrame...\")\n",
    "    results_df = pd.DataFrame(results, columns=['highest_individual_name', 'highest_individual_score'])\n",
    "    results_df['true_label'] = test_df[label_column].tolist()\n",
    "\n",
    "    print(\"Calculating accuracy...\")\n",
    "    accuracy_individual = (results_df['highest_individual_name'].astype(str) == results_df['true_label'].astype(str)).mean()\n",
    "\n",
    "    print(f\"Accuracy based on highest individual cosine similarity: {accuracy_individual * 100:.2f}%\")\n",
    "\n",
    "    return results_df, accuracy_individual\n",
    "\n",
    "print(\"Starting batch processing for predictions...\")\n",
    "label_column = 'scientificName' \n",
    "results_df, accuracy_individual = apply_best_match_with_batches(\n",
    "    test_embeddings_normalized, train_embeddings_normalized, train_df, label_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4085fd4-abda-4ea6-8780-4bb97ae6dc9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f44686-e5ce-4e06-96d4-77ab14300f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f2636a0-fbd2-41a4-aa86-8f44eaba72cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.0104,  0.0348,  ..., -0.0025, -0.0132, -0.0383],\n",
       "        [-0.0163, -0.0357,  0.0053,  ...,  0.0172, -0.0041,  0.0042],\n",
       "        [-0.0548,  0.0020,  0.0155,  ...,  0.0040, -0.0031, -0.0182],\n",
       "        ...,\n",
       "        [ 0.0287,  0.0106,  0.0475,  ..., -0.0117,  0.0215, -0.0833],\n",
       "        [ 0.0435,  0.0472, -0.0168,  ..., -0.0033,  0.0178, -0.0201],\n",
       "        [-0.0067,  0.0365,  0.0133,  ..., -0.0101,  0.0015, -0.0598]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c842504e-6332-4c61-bfdf-deff2733385e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16776a-d994-4365-bef9-51d606dbe2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67a615-62a9-4988-b134-6a5bbe64431e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81e967-8a11-44bc-834d-2bb2fe3b28e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8972a8e3-548e-4ae2-b69c-abbe2f223d10",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d153f92-3268-4a08-9f1a-cd481ac6aea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14-378-quickgelu', pretrained='dfn5b')\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "# Ensure the model is on the same device as the inputs\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14-378-quickgelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabb241-0c07-4d4a-9c73-0128285c7e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_multiprocessing(df_cleaned, preprocess, model, device, num_workers=4):\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    image_filenames = df_cleaned['image_filename'].tolist()\n",
    "    num_images = len(image_filenames)\n",
    "    result_list = mp.Manager().list([None] * num_images)\n",
    "    progress_counter = mp.Value('i', 0)  # Shared counter for tracking progress\n",
    "\n",
    "    batch_size = num_images // num_workers\n",
    "\n",
    "    processes = []\n",
    "    for worker_id in range(num_workers):\n",
    "        start_idx = worker_id * batch_size\n",
    "        end_idx = (worker_id + 1) * batch_size if worker_id < num_workers - 1 else num_images\n",
    "        process = mp.Process(target=worker_function, args=(image_filenames, start_idx, end_idx, result_list, preprocess, model, device, progress_counter))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    with tqdm(total=num_images, desc=\"Getting Image Embeddings\") as pbar:\n",
    "        while any(p.is_alive() for p in processes):\n",
    "            with progress_counter.get_lock():\n",
    "                pbar.n = progress_counter.value\n",
    "            pbar.refresh()\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    df_cleaned['image_embeddings'] = list(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621aa3c-fe2f-4c27-bea4-deb54c993563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Image Embeddings:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 10788/11092 [12:07<00:20, 14.83it/s]     "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Running the multiprocessing function in Jupyter\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "run_multiprocessing(df_cleaned, preprocess, model, device, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb475d4a-ff94-408c-8932-8b2cde99c182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (7431, 4)\n",
      "Test set size: (3661, 4)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import open_clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import  unicom\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "import faiss\n",
    "# Function to extract the genus from the scientific name (first part before space)\n",
    "df_cleaned = pd.read_pickle('df_cleaned.pkl')\n",
    "def extract_genus(scientific_name):\n",
    "    return scientific_name.split()[0]\n",
    "\n",
    "def split_create_genus():\n",
    "    # Apply genus extraction to df_cleaned\n",
    "    global train_df, test_df\n",
    "    df_cleaned['genus'] = df_cleaned['scientificName'].apply(extract_genus)\n",
    "\n",
    "    # Count the number of samples per genus\n",
    "    genus_counts = df_cleaned['genus'].value_counts()\n",
    "\n",
    "    # Splitting the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n",
    "\n",
    "    # Checking the resulting shapes of the train and test sets\n",
    "    print(f\"Training set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    \n",
    "split_create_genus()\n",
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7a241-c0e1-4ebf-91e0-4a32fa0cf98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns=['image_embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5fe87-57d9-43fd-a48a-04b3a4e279a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each image in your dataframe and store embeddings in a new column\n",
    "df_cleaned['image_embeddings'] = df_cleaned['image_filename'].apply(lambda x: get_image_embeddings(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9b5ea-15e4-44c3-a44c-b2fce2086490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_create_genus()\n",
    "\n",
    "apply_best_match(test_df, df_cleaned, 'genus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0be1ba-0090-47d7-a1eb-5641920569a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_torch_tensors\u001b[39m(train_df, test_df, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1: Converting train and test embeddings to torch tensors...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     train_embeddings_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m      4\u001b[0m         np\u001b[38;5;241m.\u001b[39mstack(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_to_torch_tensors(train_df, test_df, device='cuda', dtype=torch.float16):\n",
    "    print(\"Step 1: Converting train and test embeddings to torch tensors...\")\n",
    "    train_embeddings_tensor = torch.tensor(\n",
    "        np.stack(train_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    test_embeddings_tensor = torch.tensor(\n",
    "        np.stack(test_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    return train_embeddings_tensor, test_embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c7866e-b359-45ad-a93e-e3c0f37379dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Converting train and test embeddings to torch tensors...\n",
      "Step 2: Reshaping the embeddings...\n",
      "Starting batch processing for predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Embeddings in Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 82.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch from index 0 to 512...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 512 to 1024...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 1024 to 1536...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 1536 to 2048...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 2048 to 2560...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 2560 to 3072...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 3072 to 3584...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Processing batch from index 3584 to 3661...\n",
      "Calculating cosine similarities...\n",
      "Finding highest similarity indices...\n",
      "Appending results...\n",
      "Clearing CUDA memory...\n",
      "Creating results DataFrame...\n",
      "Calculating accuracy...\n",
      "Accuracy based on highest individual cosine similarity: 84.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor):\n",
    "    print(\"Step 2: Reshaping the embeddings...\")\n",
    "    train_embeddings_reshaped = train_embeddings_tensor.view(train_embeddings_tensor.size(0), -1)\n",
    "    test_embeddings_reshaped = test_embeddings_tensor.view(test_embeddings_tensor.size(0), -1)\n",
    "    return train_embeddings_reshaped, test_embeddings_reshaped\n",
    "\n",
    "train_embeddings_tensor, test_embeddings_tensor = convert_to_torch_tensors(train_df, test_df)\n",
    "\n",
    "train_embeddings_reshaped, test_embeddings_reshaped = reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor)\n",
    "\n",
    "epsilon = 1e-8\n",
    "train_embeddings_normalized = train_embeddings_reshaped / (train_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "test_embeddings_normalized = test_embeddings_reshaped / (test_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "\n",
    "def apply_best_match_with_batches(test_embeddings_normalized, train_embeddings_normalized, train_df, label_column, batch_size=512):\n",
    "    train_labels = train_df[label_column].tolist()\n",
    "    num_test_samples = test_embeddings_normalized.size(0)\n",
    "    results = []\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_test_samples, batch_size), desc=\"Processing Test Embeddings in Batches\"):\n",
    "        print(f\"Processing batch from index {start_idx} to {min(start_idx + batch_size, num_test_samples)}...\")\n",
    "        end_idx = min(start_idx + batch_size, num_test_samples)\n",
    "        batch_embeddings = test_embeddings_normalized[start_idx:end_idx]\n",
    "\n",
    "        print(\"Calculating cosine similarities...\")\n",
    "        cosine_similarities = torch.matmul(batch_embeddings, train_embeddings_normalized.T)\n",
    "\n",
    "        print(\"Finding highest similarity indices...\")\n",
    "        max_similarity_indices = torch.argmax(cosine_similarities, dim=1)\n",
    "        max_similarity_values = torch.max(cosine_similarities, dim=1).values\n",
    "\n",
    "        print(\"Appending results...\")\n",
    "        for idx, sim_value in zip(max_similarity_indices, max_similarity_values):\n",
    "            results.append((train_labels[idx.item()], sim_value.item()))\n",
    "\n",
    "        print(\"Clearing CUDA memory...\")\n",
    "        del cosine_similarities\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Creating results DataFrame...\")\n",
    "    results_df = pd.DataFrame(results, columns=['highest_individual_name', 'highest_individual_score'])\n",
    "    results_df['true_label'] = test_df[label_column].tolist()\n",
    "\n",
    "    print(\"Calculating accuracy...\")\n",
    "    accuracy_individual = (results_df['highest_individual_name'].astype(str) == results_df['true_label'].astype(str)).mean()\n",
    "\n",
    "    print(f\"Accuracy based on highest individual cosine similarity: {accuracy_individual * 100:.2f}%\")\n",
    "\n",
    "    return results_df, accuracy_individual\n",
    "\n",
    "print(\"Starting batch processing for predictions...\")\n",
    "label_column = 'scientificName'\n",
    "results_df, accuracy_individual = apply_best_match_with_batches(\n",
    "    test_embeddings_normalized, train_embeddings_normalized, train_df, label_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd1044-c234-46d0-82e2-5ccc097857b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bio Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655da497-02db-4957-a324-d6d3f1893ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:imageomics/bioclip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d1dff32-0ba0-4ab7-a6d3-a64c8f648cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns=['image_embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580338b8-df60-4b53-b841-5d7aa419e6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:00<?, ?it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:01<?, ?it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:02<?, ?it/s]Process Process-13:\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:02<?, ?it/s]Traceback (most recent call last):\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:02<?, ?it/s]  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ahmed.waseem/Bio_vision_lab/embedding_utils.py\", line 25, in worker_function\n",
      "    embedding = get_image_embeddings(image_filenames[idx], preprocess, model, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'get_image_embeddings' is not defined\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:02<?, ?it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Getting Image Embeddings:   0%|          | 0/11092 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Running the multiprocessing function in Jupyter\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "run_multiprocessing(df_cleaned, preprocess, model, device, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd7cf00-4d06-447f-adde-cfc7975dc79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split_create_genus()\n",
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d13395-a34e-4032-adda-e2dc6667ce03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Converting train and test embeddings to torch tensors...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     test_embeddings_reshaped \u001b[38;5;241m=\u001b[39m test_embeddings_tensor\u001b[38;5;241m.\u001b[39mview(test_embeddings_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_embeddings_reshaped, test_embeddings_reshaped\n\u001b[0;32m---> 17\u001b[0m train_embeddings_tensor, test_embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_torch_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m train_embeddings_reshaped, test_embeddings_reshaped \u001b[38;5;241m=\u001b[39m reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor)\n\u001b[1;32m     21\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m, in \u001b[0;36mconvert_to_torch_tensors\u001b[0;34m(train_df, test_df, device, dtype)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_torch_tensors\u001b[39m(train_df, test_df, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1: Converting train and test embeddings to torch tensors...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train_embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     test_embeddings_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m      7\u001b[0m         np\u001b[38;5;241m.\u001b[39mstack(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_embeddings_tensor, test_embeddings_tensor\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "def convert_to_torch_tensors(train_df, test_df, device='cuda', dtype=torch.float16):\n",
    "    print(\"Step 1: Converting train and test embeddings to torch tensors...\")\n",
    "    train_embeddings_tensor = torch.tensor(\n",
    "        np.stack(train_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    test_embeddings_tensor = torch.tensor(\n",
    "        np.stack(test_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    return train_embeddings_tensor, test_embeddings_tensor\n",
    "\n",
    "def reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor):\n",
    "    print(\"Step 2: Reshaping the embeddings...\")\n",
    "    train_embeddings_reshaped = train_embeddings_tensor.view(train_embeddings_tensor.size(0), -1)\n",
    "    test_embeddings_reshaped = test_embeddings_tensor.view(test_embeddings_tensor.size(0), -1)\n",
    "    return train_embeddings_reshaped, test_embeddings_reshaped\n",
    "\n",
    "train_embeddings_tensor, test_embeddings_tensor = convert_to_torch_tensors(train_df, test_df)\n",
    "\n",
    "train_embeddings_reshaped, test_embeddings_reshaped = reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor)\n",
    "\n",
    "epsilon = 1e-8\n",
    "train_embeddings_normalized = train_embeddings_reshaped / (train_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "test_embeddings_normalized = test_embeddings_reshaped / (test_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "\n",
    "def apply_best_match_with_batches(test_embeddings_normalized, train_embeddings_normalized, train_df, label_column, batch_size=512):\n",
    "    train_labels = train_df[label_column].tolist()\n",
    "    num_test_samples = test_embeddings_normalized.size(0)\n",
    "    results = []\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_test_samples, batch_size), desc=\"Processing Test Embeddings in Batches\"):\n",
    "        print(f\"Processing batch from index {start_idx} to {min(start_idx + batch_size, num_test_samples)}...\")\n",
    "        end_idx = min(start_idx + batch_size, num_test_samples)\n",
    "        batch_embeddings = test_embeddings_normalized[start_idx:end_idx]\n",
    "\n",
    "        print(\"Calculating cosine similarities...\")\n",
    "        cosine_similarities = torch.matmul(batch_embeddings, train_embeddings_normalized.T)\n",
    "\n",
    "        print(\"Finding highest similarity indices...\")\n",
    "        max_similarity_indices = torch.argmax(cosine_similarities, dim=1)\n",
    "        max_similarity_values = torch.max(cosine_similarities, dim=1).values\n",
    "\n",
    "        print(\"Appending results...\")\n",
    "        for idx, sim_value in zip(max_similarity_indices, max_similarity_values):\n",
    "            results.append((train_labels[idx.item()], sim_value.item()))\n",
    "\n",
    "        print(\"Clearing CUDA memory...\")\n",
    "        del cosine_similarities\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Creating results DataFrame...\")\n",
    "    results_df = pd.DataFrame(results, columns=['highest_individual_name', 'highest_individual_score'])\n",
    "    results_df['true_label'] = test_df[label_column].tolist()\n",
    "\n",
    "    print(\"Calculating accuracy...\")\n",
    "    accuracy_individual = (results_df['highest_individual_name'].astype(str) == results_df['true_label'].astype(str)).mean()\n",
    "\n",
    "    print(f\"Accuracy based on highest individual cosine similarity: {accuracy_individual * 100:.2f}%\")\n",
    "\n",
    "    return results_df, accuracy_individual\n",
    "\n",
    "print(\"Starting batch processing for predictions...\")\n",
    "label_column = 'scientificName'\n",
    "results_df, accuracy_individual = apply_best_match_with_batches(\n",
    "    test_embeddings_normalized, train_embeddings_normalized, train_df, label_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2478ed82-34a7-4d80-946a-3f6c5ebae974",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_filename</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>genus</th>\n",
       "      <th>image_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>Green_Tailed_Towhee_0027_154823.jpg</td>\n",
       "      <td>Pipilo chlorurus</td>\n",
       "      <td>Pipilo</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>Bank_Swallow_0053_129501.jpg</td>\n",
       "      <td>Riparia riparia</td>\n",
       "      <td>Riparia</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>Bay_Breasted_Warbler_0045_797135.jpg</td>\n",
       "      <td>Setophaga castanea</td>\n",
       "      <td>Setophaga</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9207</th>\n",
       "      <td>Kentucky_Warbler_0027_795917.jpg</td>\n",
       "      <td>Geothlypis formosa</td>\n",
       "      <td>Geothlypis</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Yellow_Headed_Blackbird_0083_8300.jpg</td>\n",
       "      <td>Xanthocephalus xanthocephalus</td>\n",
       "      <td>Xanthocephalus</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>Swainson_Warbler_0020_794863.jpg</td>\n",
       "      <td>Limnothlypis swainsonii</td>\n",
       "      <td>Limnothlypis</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>Red_Faced_Cormorant_0066_796333.jpg</td>\n",
       "      <td>Phalacrocorax urile</td>\n",
       "      <td>Phalacrocorax</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>Pomarine_Jaeger_0062_61351.jpg</td>\n",
       "      <td>Stercorarius pomarinus</td>\n",
       "      <td>Stercorarius</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7223</th>\n",
       "      <td>Cape_Glossy_Starling_0060_129222.jpg</td>\n",
       "      <td>Lamprotornis nitens</td>\n",
       "      <td>Lamprotornis</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>Carolina_Wren_0068_186830.jpg</td>\n",
       "      <td>Thryothorus ludovicianus</td>\n",
       "      <td>Thryothorus</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3661 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_filename                 scientificName  \\\n",
       "8039     Green_Tailed_Towhee_0027_154823.jpg               Pipilo chlorurus   \n",
       "7275            Bank_Swallow_0053_129501.jpg                Riparia riparia   \n",
       "8632    Bay_Breasted_Warbler_0045_797135.jpg             Setophaga castanea   \n",
       "9207        Kentucky_Warbler_0027_795917.jpg             Geothlypis formosa   \n",
       "629    Yellow_Headed_Blackbird_0083_8300.jpg  Xanthocephalus xanthocephalus   \n",
       "...                                      ...                            ...   \n",
       "9772        Swainson_Warbler_0020_794863.jpg        Limnothlypis swainsonii   \n",
       "1277     Red_Faced_Cormorant_0066_796333.jpg            Phalacrocorax urile   \n",
       "4007          Pomarine_Jaeger_0062_61351.jpg         Stercorarius pomarinus   \n",
       "7223    Cape_Glossy_Starling_0060_129222.jpg            Lamprotornis nitens   \n",
       "10784          Carolina_Wren_0068_186830.jpg       Thryothorus ludovicianus   \n",
       "\n",
       "                genus image_embeddings  \n",
       "8039           Pipilo             None  \n",
       "7275          Riparia             None  \n",
       "8632        Setophaga             None  \n",
       "9207       Geothlypis             None  \n",
       "629    Xanthocephalus             None  \n",
       "...               ...              ...  \n",
       "9772     Limnothlypis             None  \n",
       "1277    Phalacrocorax             None  \n",
       "4007     Stercorarius             None  \n",
       "7223     Lamprotornis             None  \n",
       "10784     Thryothorus             None  \n",
       "\n",
       "[3661 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb41f4-fba7-4e55-98bf-2a58d04643bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45316017-7e36-422e-8012-a9c73f61553a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786967f-8ff5-42c8-8949-46ced4d05274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicom\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 1: Load the UNICOM model\n",
    "model_name = \"ViT-L/14@336px\"\n",
    "model, preprocess = unicom.load(model_name)\n",
    "model.eval()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd4579-25db-41bb-862e-dbeda5e53ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962abc4f-3b41-4d00-807d-e73609508dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns=['image_embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d924ea9-b008-4ea6-aa58-a4c48dd98ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running the multiprocessing function in Jupyter\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "run_multiprocessing(df_cleaned, preprocess, model, device, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b759ae2-4d48-4978-97d3-1c6bb2ce1963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_filename):\n",
    "    # Folder where images are stored\n",
    "    image_folder = \"downloaded_images\"\n",
    "    \n",
    "    try:\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        # Open the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Prepare the image using the CLIP processor (preprocessing)\n",
    "        inputs = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():  # Avoid computing gradients since we are not training\n",
    "            image_embeddings = model(inputs)\n",
    "        \n",
    "        # Convert embeddings to a CPU tensor (to avoid keeping them on GPU)\n",
    "        return image_embeddings.cpu().numpy()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None  # Return None if the image could not be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f303fc65-dbb2-4346-9e1c-4b1f5159ef56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_filename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_filename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m, in \u001b[0;36mget_image_embeddings\u001b[0;34m(image_filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Avoid computing gradients since we are not training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert embeddings to a CPU tensor (to avoid keeping them on GPU)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_embeddings\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/unicom/vision_transformer.py:57\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature(x)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/unicom/vision_transformer.py:52\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mreshape(x, (B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed\u001b[38;5;241m.\u001b[39mnum_patches \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim))\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/unicom/vision_transformer.py:122\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_checkpoint:\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_impl(x)\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/utils/checkpoint.py:487\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         )\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    490\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    491\u001b[0m     )\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/utils/checkpoint.py:262\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    259\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/unicom/vision_transformer.py:116\u001b[0m, in \u001b[0;36mBlock.forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 116\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    117\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/unicom/vision_transformer.py:86\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m     87\u001b[0m         B, L, D \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     88\u001b[0m         qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, L, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m     89\u001b[0m                                   D \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/cuda/amp/autocast_mode.py:34\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, enabled, dtype, cache_enabled)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_enabled\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/bvl/lib/python3.12/site-packages/torch/amp/autocast_mode.py:202\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device_type\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_backend_name \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_privateuse1_backend_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_autocast_gpu_dtype()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_cleaned['image_embeddings'] = df_cleaned['image_filename'].apply(lambda x: get_image_embeddings(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed6bb0-aafb-4b40-aee5-79561a9cb083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_create_genus()\n",
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1e378-ae5f-403b-9af9-7bbb45b4ac7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_torch_tensors(train_df, test_df, device='cuda', dtype=torch.float16):\n",
    "    print(\"Step 1: Converting train and test embeddings to torch tensors...\")\n",
    "    train_embeddings_tensor = torch.tensor(\n",
    "        np.stack(train_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    test_embeddings_tensor = torch.tensor(\n",
    "        np.stack(test_df['image_embeddings'].values), device=device, dtype=dtype\n",
    "    )\n",
    "    return train_embeddings_tensor, test_embeddings_tensor\n",
    "\n",
    "def reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor):\n",
    "    print(\"Step 2: Reshaping the embeddings...\")\n",
    "    train_embeddings_reshaped = train_embeddings_tensor.view(train_embeddings_tensor.size(0), -1)\n",
    "    test_embeddings_reshaped = test_embeddings_tensor.view(test_embeddings_tensor.size(0), -1)\n",
    "    return train_embeddings_reshaped, test_embeddings_reshaped\n",
    "\n",
    "train_embeddings_tensor, test_embeddings_tensor = convert_to_torch_tensors(train_df, test_df)\n",
    "\n",
    "train_embeddings_reshaped, test_embeddings_reshaped = reshape_embeddings(train_embeddings_tensor, test_embeddings_tensor)\n",
    "\n",
    "epsilon = 1e-8\n",
    "train_embeddings_normalized = train_embeddings_reshaped / (train_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "test_embeddings_normalized = test_embeddings_reshaped / (test_embeddings_reshaped.norm(dim=1, keepdim=True) + epsilon)\n",
    "\n",
    "def apply_best_match_with_batches(test_embeddings_normalized, train_embeddings_normalized, train_df, label_column, batch_size=512):\n",
    "    train_labels = train_df[label_column].tolist()\n",
    "    num_test_samples = test_embeddings_normalized.size(0)\n",
    "    results = []\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_test_samples, batch_size), desc=\"Processing Test Embeddings in Batches\"):\n",
    "        print(f\"Processing batch from index {start_idx} to {min(start_idx + batch_size, num_test_samples)}...\")\n",
    "        end_idx = min(start_idx + batch_size, num_test_samples)\n",
    "        batch_embeddings = test_embeddings_normalized[start_idx:end_idx]\n",
    "\n",
    "        print(\"Calculating cosine similarities...\")\n",
    "        cosine_similarities = torch.matmul(batch_embeddings, train_embeddings_normalized.T)\n",
    "\n",
    "        print(\"Finding highest similarity indices...\")\n",
    "        max_similarity_indices = torch.argmax(cosine_similarities, dim=1)\n",
    "        max_similarity_values = torch.max(cosine_similarities, dim=1).values\n",
    "\n",
    "        print(\"Appending results...\")\n",
    "        for idx, sim_value in zip(max_similarity_indices, max_similarity_values):\n",
    "            results.append((train_labels[idx.item()], sim_value.item()))\n",
    "\n",
    "        print(\"Clearing CUDA memory...\")\n",
    "        del cosine_similarities\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Creating results DataFrame...\")\n",
    "    results_df = pd.DataFrame(results, columns=['highest_individual_name', 'highest_individual_score'])\n",
    "    results_df['true_label'] = test_df[label_column].tolist()\n",
    "\n",
    "    print(\"Calculating accuracy...\")\n",
    "    accuracy_individual = (results_df['highest_individual_name'].astype(str) == results_df['true_label'].astype(str)).mean()\n",
    "\n",
    "    print(f\"Accuracy based on highest individual cosine similarity: {accuracy_individual * 100:.2f}%\")\n",
    "\n",
    "    return results_df, accuracy_individual\n",
    "\n",
    "print(\"Starting batch processing for predictions...\")\n",
    "label_column = 'scientificName'\n",
    "results_df, accuracy_individual = apply_best_match_with_batches(\n",
    "    test_embeddings_normalized, train_embeddings_normalized, train_df, label_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96209e9-ec18-46af-9ea0-08609900b4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_images_with_scientific_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49bf1e-5e52-47f7-97fb-455db84fbceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_genus(scientific_name):\n",
    "    return scientific_name.split()[0]\n",
    "\n",
    "df['genus'] = df['scientificName'].apply(extract_genus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23f57d-e51c-4c23-97f3-ee7ab666ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioVision",
   "language": "python",
   "name": "bvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
