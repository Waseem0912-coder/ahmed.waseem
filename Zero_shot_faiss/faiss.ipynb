{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is correctly set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import open_clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import  unicom\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "import faiss\n",
    "import gc\n",
    "from float_converter import convert_embeddings_column_to_float32, index_faiss_cosine_similarity, search_faiss_index\n",
    "#from image_utils import worker_function, get_image_embeddings\n",
    "#from embedding_utils import convert_to_tensors, mean_pool_embeddings, normalize_embeddings, worker_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"cleaned_images_with_scientific_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the genus from the scientific name (first part before space)\n",
    "global train_df \n",
    "global test_df\n",
    "\n",
    "def extract_genus(scientific_name):\n",
    "    return scientific_name.split()[0]\n",
    "\n",
    "def split_create_genus():\n",
    "    # Apply genus extraction to df_cleaned\n",
    "    global train_df, test_df\n",
    "    df_cleaned['genus'] = df_cleaned['scientificName'].apply(extract_genus)\n",
    "\n",
    "    # Count the number of samples per genus\n",
    "    genus_counts = df_cleaned['genus'].value_counts()\n",
    "\n",
    "    # Splitting the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n",
    "\n",
    "    # Checking the resulting shapes of the train and test sets\n",
    "    print(f\"Training set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Set up device and torch dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load Florence-2 Large model and processor\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "model = model.eval()\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for handling image loading\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_filenames, image_folder):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.image_folder = image_folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_filename)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return image, image_filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing image {image_filename}: {e}\")\n",
    "            return None, image_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    images, filenames = zip(*batch)\n",
    "    # Filter out None images from the batch\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_filenames = [fname for img, fname in zip(images, filenames) if img is not None]\n",
    "\n",
    "    if len(valid_images) > 0:\n",
    "        # Pass the raw images directly to the processor here (on CPU)\n",
    "        inputs = processor(images=valid_images, return_tensors=\"pt\")\n",
    "        # Ensure the input tensor is in float16 if the model expects it\n",
    "        inputs = {k: v.to(dtype=torch.float16) if model.dtype == torch.float16 else v for k, v in inputs.items()}\n",
    "        return inputs, valid_filenames\n",
    "    else:\n",
    "        return None, valid_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image embeddings with tqdm progress bar and dtype correction\n",
    "def image_embedding(df, batch_size=16, num_workers=3):  \n",
    "    image_filenames = df['image_filename'].tolist()\n",
    "    image_folder = \"Zero_shot_faiss/downloaded_images\"\n",
    "    \n",
    "    dataset = ImageDataset(image_filenames, image_folder)\n",
    "    \n",
    "    # Create DataLoader with the custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    results = []\n",
    "    filenames = []\n",
    "    \n",
    "    with tqdm(total=len(dataloader), desc=\"Processing Batches\", unit=\"batch\") as pbar:\n",
    "        for inputs, batch_filenames in dataloader:\n",
    "            if inputs is not None:\n",
    "                with torch.inference_mode():\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    embeddings = model._encode_image(inputs[\"pixel_values\"]).cpu().numpy()\n",
    "                    clear_memory()\n",
    "                    results.append(embeddings)\n",
    "                    filenames.extend(batch_filenames)\n",
    "                    clear_memory()\n",
    "            pbar.update(1)  \n",
    "                \n",
    "    chunk_size = 1000  # You can adjust this based on your memory limitations\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(results), chunk_size):\n",
    "        chunk = np.vstack(results[i:i + chunk_size])\n",
    "        all_embeddings.extend(chunk)\n",
    "\n",
    "    df['image_embeddings'] = all_embeddings\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4f3cfc868941d99fa4070b50db3a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/694 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n"
     ]
    }
   ],
   "source": [
    "df = df_cleaned \n",
    "df = image_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (7431, 4)\n",
      "Test set size: (3661, 4)\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "clear_memory()\n",
    "df_cleaned.to_pickle(\"df_cleaned.pkl\")\n",
    "split_create_genus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.read_pickle(\"df_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion of 11092 embeddings from float16 to float32 in batches of 100\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = convert_embeddings_column_to_float32(df_cleaned, \"image_embeddings\", batch_size=100, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioVision",
   "language": "python",
   "name": "bvl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
