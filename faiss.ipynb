{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is correctly set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaiss_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_embeddings_to_float32, index_faiss_cosine_similarity, search_faiss_index\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#from image_utils import worker_function, get_image_embeddings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#from embedding_utils import convert_to_tensors, mean_pool_embeddings, normalize_embeddings, worker_function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss_helper'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import open_clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import  unicom\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "import torch.multiprocessing as mp\n",
    "import faiss\n",
    "import gc\n",
    "from float_converter import convert_embeddings_to_float32, index_faiss_cosine_similarity, search_faiss_index\n",
    "#from image_utils import worker_function, get_image_embeddings\n",
    "#from embedding_utils import convert_to_tensors, mean_pool_embeddings, normalize_embeddings, worker_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"cleaned_images_with_scientific_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the genus from the scientific name (first part before space)\n",
    "global train_df \n",
    "global test_df\n",
    "\n",
    "def extract_genus(scientific_name):\n",
    "    return scientific_name.split()[0]\n",
    "\n",
    "def split_create_genus():\n",
    "    # Apply genus extraction to df_cleaned\n",
    "    global train_df, test_df\n",
    "    df_cleaned['genus'] = df_cleaned['scientificName'].apply(extract_genus)\n",
    "\n",
    "    # Count the number of samples per genus\n",
    "    genus_counts = df_cleaned['genus'].value_counts()\n",
    "\n",
    "    # Splitting the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df_cleaned, test_size=0.33, random_state=432, stratify=df_cleaned['genus'])\n",
    "\n",
    "    # Checking the resulting shapes of the train and test sets\n",
    "    print(f\"Training set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Set up device and torch dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load Florence-2 Large model and processor\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "model = model.eval()\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for handling image loading\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_filenames, image_folder):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.image_folder = image_folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_filename)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return image, image_filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing image {image_filename}: {e}\")\n",
    "            return None, image_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    images, filenames = zip(*batch)\n",
    "    # Filter out None images from the batch\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_filenames = [fname for img, fname in zip(images, filenames) if img is not None]\n",
    "\n",
    "    if len(valid_images) > 0:\n",
    "        # Pass the raw images directly to the processor here (on CPU)\n",
    "        inputs = processor(images=valid_images, return_tensors=\"pt\")\n",
    "        # Ensure the input tensor is in float16 if the model expects it\n",
    "        inputs = {k: v.to(dtype=torch.float16) if model.dtype == torch.float16 else v for k, v in inputs.items()}\n",
    "        return inputs, valid_filenames\n",
    "    else:\n",
    "        return None, valid_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image embeddings with tqdm progress bar and dtype correction\n",
    "def image_embedding(df, batch_size=16, num_workers=3):  \n",
    "    image_filenames = df['image_filename'].tolist()\n",
    "    image_folder = \"Zero_shot_faiss/downloaded_images\"\n",
    "    \n",
    "    dataset = ImageDataset(image_filenames, image_folder)\n",
    "    \n",
    "    # Create DataLoader with the custom collate function\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    results = []\n",
    "    filenames = []\n",
    "    \n",
    "    with tqdm(total=len(dataloader), desc=\"Processing Batches\", unit=\"batch\") as pbar:\n",
    "        for inputs, batch_filenames in dataloader:\n",
    "            if inputs is not None:\n",
    "                with torch.inference_mode():\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    embeddings = model._encode_image(inputs[\"pixel_values\"]).cpu().numpy()\n",
    "                    clear_memory()\n",
    "                    results.append(embeddings)\n",
    "                    filenames.extend(batch_filenames)\n",
    "                    clear_memory()\n",
    "            pbar.update(1)  \n",
    "                \n",
    "    chunk_size = 1000  # You can adjust this based on your memory limitations\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(results), chunk_size):\n",
    "        chunk = np.vstack(results[i:i + chunk_size])\n",
    "        all_embeddings.extend(chunk)\n",
    "\n",
    "    df['image_embeddings'] = all_embeddings\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4f3cfc868941d99fa4070b50db3a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/694 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n",
      "You are using Florence-2 without a text prompt.\n"
     ]
    }
   ],
   "source": [
    "df = df_cleaned \n",
    "df = image_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (7431, 4)\n",
      "Test set size: (3661, 4)\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "clear_memory()\n",
    "df_cleaned.to_pickle(\"df_cleaned.pkl\")\n",
    "split_create_genus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_filename</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>image_embeddings</th>\n",
       "      <th>genus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black_Footed_Albatross_0046_18.jpg</td>\n",
       "      <td>Phoebastria nigripes</td>\n",
       "      <td>[[-0.8975, -1.191, 0.494, -1.011, 0.959, 0.512...</td>\n",
       "      <td>Phoebastria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black_Footed_Albatross_0009_34.jpg</td>\n",
       "      <td>Phoebastria nigripes</td>\n",
       "      <td>[[-0.3833, -0.7427, 0.923, -1.299, 0.7534, 0.8...</td>\n",
       "      <td>Phoebastria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black_Footed_Albatross_0002_55.jpg</td>\n",
       "      <td>Phoebastria nigripes</td>\n",
       "      <td>[[-0.5386, -1.347, 0.2593, -1.133, 0.7114, 0.7...</td>\n",
       "      <td>Phoebastria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black_Footed_Albatross_0074_59.jpg</td>\n",
       "      <td>Phoebastria nigripes</td>\n",
       "      <td>[[-0.6616, -0.9644, 0.9985, -1.19, 0.612, 0.69...</td>\n",
       "      <td>Phoebastria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black_Footed_Albatross_0014_89.jpg</td>\n",
       "      <td>Phoebastria nigripes</td>\n",
       "      <td>[[-0.7295, -0.881, 1.014, -1.024, 1.091, 0.636...</td>\n",
       "      <td>Phoebastria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11087</th>\n",
       "      <td>Common_Yellowthroat_0037_190698.jpg</td>\n",
       "      <td>Geothlypis trichas</td>\n",
       "      <td>[[-0.1417, 0.4326, 0.0938, 0.1576, 0.1501, 0.5...</td>\n",
       "      <td>Geothlypis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11088</th>\n",
       "      <td>Common_Yellowthroat_0058_190958.jpg</td>\n",
       "      <td>Geothlypis trichas</td>\n",
       "      <td>[[0.2878, 0.3975, 0.4333, 0.2947, 0.1512, 0.91...</td>\n",
       "      <td>Geothlypis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11089</th>\n",
       "      <td>Common_Yellowthroat_0008_190703.jpg</td>\n",
       "      <td>Geothlypis trichas</td>\n",
       "      <td>[[0.426, 0.41, 0.338, 0.2316, 0.319, 1.218, 0....</td>\n",
       "      <td>Geothlypis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11090</th>\n",
       "      <td>Common_Yellowthroat_0049_190708.jpg</td>\n",
       "      <td>Geothlypis trichas</td>\n",
       "      <td>[[1.083, 0.481, 0.03087, 0.423, 0.6567, 1.045,...</td>\n",
       "      <td>Geothlypis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>Common_Yellowthroat_0055_190967.jpg</td>\n",
       "      <td>Geothlypis trichas</td>\n",
       "      <td>[[0.1476, 0.4265, 0.1832, 0.05225, 0.11865, 0....</td>\n",
       "      <td>Geothlypis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11092 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image_filename        scientificName  \\\n",
       "0       Black_Footed_Albatross_0046_18.jpg  Phoebastria nigripes   \n",
       "1       Black_Footed_Albatross_0009_34.jpg  Phoebastria nigripes   \n",
       "2       Black_Footed_Albatross_0002_55.jpg  Phoebastria nigripes   \n",
       "3       Black_Footed_Albatross_0074_59.jpg  Phoebastria nigripes   \n",
       "4       Black_Footed_Albatross_0014_89.jpg  Phoebastria nigripes   \n",
       "...                                    ...                   ...   \n",
       "11087  Common_Yellowthroat_0037_190698.jpg    Geothlypis trichas   \n",
       "11088  Common_Yellowthroat_0058_190958.jpg    Geothlypis trichas   \n",
       "11089  Common_Yellowthroat_0008_190703.jpg    Geothlypis trichas   \n",
       "11090  Common_Yellowthroat_0049_190708.jpg    Geothlypis trichas   \n",
       "11091  Common_Yellowthroat_0055_190967.jpg    Geothlypis trichas   \n",
       "\n",
       "                                        image_embeddings        genus  \n",
       "0      [[-0.8975, -1.191, 0.494, -1.011, 0.959, 0.512...  Phoebastria  \n",
       "1      [[-0.3833, -0.7427, 0.923, -1.299, 0.7534, 0.8...  Phoebastria  \n",
       "2      [[-0.5386, -1.347, 0.2593, -1.133, 0.7114, 0.7...  Phoebastria  \n",
       "3      [[-0.6616, -0.9644, 0.9985, -1.19, 0.612, 0.69...  Phoebastria  \n",
       "4      [[-0.7295, -0.881, 1.014, -1.024, 1.091, 0.636...  Phoebastria  \n",
       "...                                                  ...          ...  \n",
       "11087  [[-0.1417, 0.4326, 0.0938, 0.1576, 0.1501, 0.5...   Geothlypis  \n",
       "11088  [[0.2878, 0.3975, 0.4333, 0.2947, 0.1512, 0.91...   Geothlypis  \n",
       "11089  [[0.426, 0.41, 0.338, 0.2316, 0.319, 1.218, 0....   Geothlypis  \n",
       "11090  [[1.083, 0.481, 0.03087, 0.423, 0.6567, 1.045,...   Geothlypis  \n",
       "11091  [[0.1476, 0.4265, 0.1832, 0.05225, 0.11865, 0....   Geothlypis  \n",
       "\n",
       "[11092 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioVision",
   "language": "python",
   "name": "bvl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
